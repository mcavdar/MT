{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paracrawl dataset pre-processing for fast_align "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Download Paracrawl dataset from http://opus.nlpl.eu/ and extract files.\n",
    "\n",
    "2) Read each sentence from source and target language's xml files.\n",
    "\n",
    "3) Tokenize both sentences and aggregate them to create Fast Align system's input. \n",
    "\n",
    "<b>Requirements:</b> nltk and xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extract files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://object.pouta.csc.fi/OPUS-ParaCrawl/v1/raw/en.zip\n",
    "!wget https://object.pouta.csc.fi/OPUS-ParaCrawl/v1/raw/fr.zip\n",
    "!unzip en.zip -d en\n",
    "!unzip fr.zip -d fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read sentences, tokenize and aggregate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(277):\n",
    "    print(\"Processing %s'th file\"%(i))\n",
    "    with open(\"./en/ParaCrawl/raw/en/paracrawl-zipporah0-dedup-clean.en-fr.0\"+str(i+1).zfill(3)+\".xml\", 'rb') as f:\n",
    "        d_en = xmltodict.parse(f)\n",
    "    with open(\"./fr/ParaCrawl/raw/fr/paracrawl-zipporah0-dedup-clean.en-fr.0\"+str(i+1).zfill(3)+\".xml\", 'rb') as f:\n",
    "        d_fr = xmltodict.parse(f)\n",
    "    en_text={}\n",
    "    fr_text={}\n",
    "    for line in d_fr['text']['s']:\n",
    "        fr_text[line['@id']] = line['#text']\n",
    "    for line in d_en['text']['s']:\n",
    "        en_text[line['@id']] = line['#text']\n",
    "    with open(\"./en-fr_aggregate.txt\", mode=\"a\", encoding=\"utf8\") as f:\n",
    "        for i in range(100000):\n",
    "            if str(i+1) in en_text and str(i+1) in fr_text:\n",
    "                #print(\"%s ||| %s \" %(' '.join(word_tokenize(en_text[str(i+1)], language='english')),' '.join(word_tokenize(fr_text[str(i+1)], language='french'))))\n",
    "                print(\"%s ||| %s \" %(' '.join(word_tokenize(en_text[str(i+1)], language='english')),' '.join(word_tokenize(fr_text[str(i+1)], language='french'))), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
